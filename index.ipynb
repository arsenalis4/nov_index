{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스분석 API를 활용하여 웹3 산업 관련 뉴스 인용문 및 키워드 수집\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# API 키와 요청 파라미터 설정\n",
    "api_key = \"f107ffb99e2b44b5855a322c8326efc0\"\n",
    "params = {\n",
    "    \"q\": \"메타버스\", # 검색어\n",
    "    \"sortBy\": \"relevance\", # 정렬 방식\n",
    "    \"language\": \"ko\", # 언어\n",
    "    \"pageSize\": 100, # 페이지당 결과 개수\n",
    "    \"apiKey\": api_key # API 키\n",
    "}\n",
    "\n",
    "# API 요청 및 응답 저장\n",
    "response = requests.get(\"https://newsapi.org/v2/everything\", params=params)\n",
    "data = response.json()\n",
    "\n",
    "# 응답 데이터에서 뉴스 인용문과 키워드 추출\n",
    "quotes = [] # 인용문 리스트\n",
    "keywords = [] # 키워드 리스트\n",
    "\n",
    "for article in data[\"articles\"]:\n",
    "    # 인용문 추출\n",
    "    quote = article[\"content\"]\n",
    "    if quote:\n",
    "        quotes.append(quote)\n",
    "    \n",
    "    # 키워드 추출\n",
    "    keyword = article[\"title\"]\n",
    "    if keyword:\n",
    "        keywords.append(keyword)\n",
    "\n",
    "# 인용문과 키워드 출력\n",
    "print(\"인용문 개수:\", len(quotes))\n",
    "print(\"키워드 개수:\", len(keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수집된 텍스트 데이터를 전처리하여 불필요한 문자, 공백 등을 제거하고, 형태소 분석에 필요한 데이터만 추출\n",
    "import re\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess(text):\n",
    "    # 불필요한 문자 제거\n",
    "    text = re.sub(r\"[^가-힣A-Za-z0-9 ]\", \"\", text)\n",
    "    # 공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # 양쪽 공백 제거\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# 전처리된 인용문과 키워드 리스트 생성\n",
    "preprocessed_quotes = []\n",
    "preprocessed_keywords = []\n",
    "\n",
    "for quote in quotes:\n",
    "    preprocessed_quotes.append(preprocess(quote))\n",
    "\n",
    "for keyword in keywords:\n",
    "    preprocessed_keywords.append(preprocess(keyword))\n",
    "\n",
    "# 전처리된 인용문과 키워드 출력\n",
    "print(\"전처리된 인용문 개수:\", len(preprocessed_quotes))\n",
    "print(\"전처리된 키워드 개수:\", len(preprocessed_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출된 데이터에 대해 형태소 분석기를 적용하여, 단어를 형태소 단위로 분리하고, 각 형태소에 대한 품사 태그를 부착\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분석기 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 결과를 저장할 리스트 생성\n",
    "okt_quotes = []\n",
    "okt_keywords = []\n",
    "\n",
    "# 인용문에 대해 형태소 분석 수행\n",
    "for quote in preprocessed_quotes:\n",
    "    okt_quotes.append(okt.pos(quote))\n",
    "\n",
    "# 키워드에 대해 형태소 분석 수행\n",
    "for keyword in preprocessed_keywords:\n",
    "    okt_keywords.append(okt.pos(keyword))\n",
    "\n",
    "# 형태소 분석 결과 출력\n",
    "print(\"Okt 인용문 개수:\", len(okt_quotes))\n",
    "print(\"Okt 키워드 개수:\", len(okt_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소만 저장할 리스트 생성\n",
    "okt_quotes_morphs = []\n",
    "okt_keywords_morphs = []\n",
    "\n",
    "# okt_quotes의 원소들을 반복문으로 순회\n",
    "for quote in okt_quotes:\n",
    "    # 형태소만 저장할 임시 리스트 생성\n",
    "    temp = []\n",
    "    # quote가 튜플 형태로 되어 있는 형태소와 품사 태그를 분리\n",
    "    for morph, tag in quote:\n",
    "        # 형태소만 임시 리스트에 추가\n",
    "        temp.append(morph)\n",
    "    # 임시 리스트를 okt_quotes_morphs에 추가\n",
    "    okt_quotes_morphs.append(temp)\n",
    "\n",
    "for keyword in okt_keywords:\n",
    "    # 형태소만 저장할 임시 리스트 생성\n",
    "    temp = []\n",
    "    # quote가 튜플 형태로 되어 있는 형태소와 품사 태그를 분리\n",
    "    for morph, tag in keyword:\n",
    "        # 형태소만 임시 리스트에 추가\n",
    "        temp.append(morph)\n",
    "    # 임시 리스트를 okt_quotes_morphs에 추가\n",
    "    okt_keywords_morphs.append(temp)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Okt 인용문 형태소 개수:\", len(okt_quotes_morphs))\n",
    "print(\"Okt 인용문 형태소:\", okt_quotes_morphs)\n",
    "print(\"Okt 키워드 형태소 개수:\", len(okt_keywords_morphs))\n",
    "print(\"Okt 키워드 형태소:\", okt_keywords_morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 분석을 위한 학습 데이터 불러오기\n",
    "# 학습 데이터는 네이버 영화 리뷰 데이터를 사용하였습니다.\n",
    "# 출처: https://github.com/e9t/nsmc\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"ratings_train.txt\", sep=\"\\t\")\n",
    "test_data = pd.read_csv(\"ratings_test.txt\", sep=\"\\t\")\n",
    "train_data = train_data.dropna(subset=[\"document\"]) # document 열에 결측치가 있는 행을 제거\n",
    "test_data = test_data.dropna(subset=[\"document\"])\n",
    "\n",
    "# 학습 데이터의 형태소 분석 결과를 저장할 리스트 생성\n",
    "train_okt = []\n",
    "test_okt = []\n",
    "\n",
    "# 학습 데이터에 대해 형태소 분석 수행\n",
    "for review in train_data[\"document\"]:\n",
    "    train_okt.append(okt.morphs(review))\n",
    "\n",
    "for review in test_data[\"document\"]:\n",
    "    test_okt.append(okt.morphs(review))\n",
    "\n",
    "# 형태소 분석 결과를 정수 인덱스로 변환하기 위한 사전 생성\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_okt)\n",
    "\n",
    "# 사전에 없는 단어는 0으로 처리하기 위해 인덱스를 1부터 시작하도록 설정\n",
    "tokenizer.word_index = {key: value + 1 for key, value in tokenizer.word_index.items()}\n",
    "tokenizer.word_index[\"<PAD>\"] = 0 # 패딩을 위한 특수 토큰\n",
    "\n",
    "# 형태소 분석 결과를 정수 인덱스로 변환\n",
    "train_sequences = tokenizer.texts_to_sequences(train_okt)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_okt)\n",
    "\n",
    "# 정수 인덱스로 변환된 데이터의 길이를 통일하기 위해 패딩 추가\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 30 # 최대 길이 설정\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\")\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "# 학습 데이터의 레이블(감성)을 numpy 배열로 변환\n",
    "import numpy as np\n",
    "\n",
    "train_labels = np.array(train_data[\"label\"])\n",
    "test_labels = np.array(test_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델을 사용하여 감정 분석 수행\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 # 단어 사전의 크기\n",
    "embedding_dim = 100 # 임베딩 차원\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_padded, train_labels, epochs=1, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 모델 평가\n",
    "model.evaluate(test_padded, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹3 산업 관련 인용문에 대해 감정 분석 수행\n",
    "quotes_padded = pad_sequences(tokenizer.texts_to_sequences(okt_quotes_morphs), maxlen=max_length, padding=\"post\")\n",
    "quotes_pred = model.predict(quotes_padded)\n",
    "quotes_pred = np.round(quotes_pred).flatten() # 예측값을 반올림하여 0(부정) 또는 1(긍정)으로 변환\n",
    "\n",
    "# 웹3 산업 관련 키워드에 대해 감정 분석 수행\n",
    "keywords_padded = pad_sequences(tokenizer.texts_to_sequences(okt_keywords_morphs), maxlen=max_length, padding=\"post\")\n",
    "keywords_pred = model.predict(keywords_padded)\n",
    "keywords_pred = np.round(keywords_pred).flatten() # 예측값을 반올림하여 0(부정) 또는 1(긍정)으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인용문에 대한 감성 지수 계산\n",
    "quotes_pos = np.sum(quotes_pred == 1) # 긍정 감성 점수\n",
    "quotes_neg = np.sum(quotes_pred == 0) # 부정 감성 점수\n",
    "quotes_score = (quotes_pos - quotes_neg) / (quotes_pos + quotes_neg) # 감성 지수\n",
    "\n",
    "# 키워드에 대한 감성 지수 계산\n",
    "keywords_pos = np.sum(keywords_pred == 1) # 긍정 감성 점수\n",
    "keywords_neg = np.sum(keywords_pred == 0) # 부정 감성 점수\n",
    "keywords_score = (keywords_pos - keywords_neg) / (keywords_pos + keywords_neg) # 감성 지수\n",
    "\n",
    "# 감성 지수 출력\n",
    "print(\"인용문의 감성 지수:\", quotes_score)\n",
    "print(\"키워드의 감성 지수:\", keywords_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
